% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/control.R
\name{optim_control}
\alias{optim_control}
\title{Optimization control parameters}
\usage{
optim_control(
  max_epochs = 2000L,
  lr_init = 0.05,
  convergence_tol = 1e-04,
  patience = 50L,
  barrier_mu = 10,
  entropy_mu = 0,
  target_eff_src = NULL,
  dual_eta = 0.05,
  alpha_init = 2,
  alpha_min = NULL,
  kernel = "power",
  use_gpu = NULL,
  device = NULL,
  dtype = "float32"
)
}
\arguments{
\item{max_epochs}{Integer. Maximum number of epochs (full passes through
all tracts). The optimizer may stop earlier if convergence is detected.
Default: 2000.}

\item{lr_init}{Numeric. Initial ADAM learning rate. Reduced automatically
via ReduceLROnPlateau when the epoch loss plateaus. Default: 0.05.}

\item{convergence_tol}{Numeric. Relative change in epoch loss below which
the optimizer considers the solution converged. Default: 1e-4.}

\item{patience}{Integer. Number of consecutive epochs with no improvement
(at minimum learning rate) before early stopping. The LR scheduler
uses \code{2 * patience} as its own patience. Default: 50.}

\item{barrier_mu}{Numeric. Strength of the log-barrier penalty that
prevents any census tract from receiving zero predicted voters.
Set to 0 to disable. Default: 10.}

\item{entropy_mu}{Numeric. Strength of the Shannon entropy penalty that
discourages diffuse weight distributions (many effective sources per
tract). Higher values push the optimizer to concentrate weights on
fewer nearby stations, reducing effective sources at the cost of
higher Poisson deviance. The penalty uses the mean entropy per
bracket-tract pair, making entropy_mu scale-independent across
different problem sizes. Set to 0 to disable. Default: 0.
Ignored when \code{target_eff_src} is set (dual ascent mode).}

\item{target_eff_src}{Numeric or NULL. Target number of effective sources
per tract. When set (not NULL), enables dual ascent: the optimizer
automatically adapts \code{entropy_mu} during training to reach this target.
Must be > 1. Mutually exclusive with manual \code{entropy_mu} tuning.
Default: NULL (disabled).}

\item{dual_eta}{Numeric. Learning rate for the dual ascent update of
\code{entropy_mu}. Controls how quickly \code{entropy_mu} adapts: higher values
mean faster adaptation but risk oscillation. Default: 0.05.}

\item{alpha_init}{Numeric scalar, vector of length n, or matrix [n x k].
Initial guess for alpha. A scalar is recycled to all tracts and
brackets. Default: 2.}

\item{alpha_min}{Numeric or NULL. Lower bound for alpha values. The
reparameterization becomes \code{alpha = alpha_min + softplus(theta)}.
If NULL (default), the bound is set based on \code{kernel}:
1 for \code{"power"} (linear-or-steeper decay), 0 for \code{"exponential"}.}

\item{kernel}{Character. Kernel function for spatial decay.
\code{"power"} (default): \eqn{K(t) = (t + \text{offset})^{-\alpha}},
the classic inverse distance weighting kernel.
\code{"exponential"}: \eqn{K(t) = \exp(-\alpha \cdot t)}, which has a
light tail (relative decay increases with distance). The exponential
kernel does not need an offset and allows \code{alpha_min = 0}.}

\item{use_gpu}{Logical or NULL. If \code{TRUE}, use GPU (CUDA or MPS). If
\code{FALSE}, use CPU. If \code{NULL} (default), reads the package option
\code{interpElections.use_gpu} (set via \code{\link[=use_gpu]{use_gpu()}}).}

\item{device}{Character or NULL. Torch device: \code{"cuda"}, \code{"mps"}, or
\code{"cpu"}. Only used when GPU is enabled. Default: NULL (auto-detect).}

\item{dtype}{Character. Torch dtype: \code{"float32"} or \code{"float64"}. Default:
\code{"float32"}. Float32 halves memory usage with negligible precision loss.}
}
\value{
A list of class \code{"interpElections_optim_control"} with one element
per parameter.
}
\description{
Creates a control object for the SGD optimizer in \code{\link[=optimize_alpha]{optimize_alpha()}}.
All parameters have sensible defaults; override only what you need.
}
\examples{
# Default settings
optim_control()

# Use GPU with more epochs
optim_control(use_gpu = TRUE, max_epochs = 5000)

# Stricter convergence
optim_control(convergence_tol = 1e-6, patience = 100)

}
\seealso{
\code{\link[=optimize_alpha]{optimize_alpha()}}, \code{\link[=routing_control]{routing_control()}}
}
