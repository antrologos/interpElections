% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/optimize.R
\name{optimize_alpha}
\alias{optimize_alpha}
\title{Find optimal decay parameters (alpha) for spatial interpolation}
\usage{
optimize_alpha(
  time_matrix,
  pop_matrix,
  source_matrix,
  row_targets = NULL,
  alpha_init = NULL,
  max_epochs = 2000L,
  lr_init = 0.05,
  use_gpu = NULL,
  device = NULL,
  dtype = "float32",
  convergence_tol = 1e-04,
  patience = 50L,
  barrier_mu = 10,
  alpha_min = 1,
  offset = 1,
  verbose = TRUE
)
}
\arguments{
\item{time_matrix}{Numeric matrix [n x m]. Raw travel times.
Rows = target zones, columns = source points.}

\item{pop_matrix}{Numeric matrix [n x k]. Known population per zone,
with k demographic groups as columns.}

\item{source_matrix}{Numeric matrix [m x k]. Known counts at source
points (e.g., registered voters by age group).}

\item{row_targets}{Numeric vector of length n, or NULL. Target row sums
for balancing. Each element specifies how much weight a zone should
attract, proportional to its share of total population. If NULL
(default), auto-computed as \code{rowSums(pop_matrix) / sum(pop_matrix) * m}.}

\item{alpha_init}{Numeric scalar, vector of length n, or matrix [n x k].
Initial guess for alpha. A scalar is recycled to all n tracts and k
brackets. A vector of length n is recycled across brackets. Default: 1.}

\item{max_epochs}{Integer. Maximum number of epochs (full passes through
all tracts). The optimizer may stop earlier if convergence is detected.
Each epoch is a single gradient step (~0.1s for 5000 tracts
on GPU), so 2000 epochs is typically under 4 minutes. Default: 2000.}

\item{lr_init}{Numeric. Initial ADAM learning rate. Reduced automatically
via ReduceLROnPlateau when the epoch loss plateaus. Default: 0.05.}

\item{use_gpu}{Logical or NULL. If \code{TRUE}, use GPU (CUDA or MPS). If
\code{FALSE}, use CPU. If \code{NULL} (default), reads the package option
\code{interpElections.use_gpu} (set via \code{\link[=use_gpu]{use_gpu()}}).}

\item{device}{Character or NULL. Torch device: \code{"cuda"}, \code{"mps"}, or
\code{"cpu"}. Only used when GPU is enabled. Default: NULL (auto-detect).}

\item{dtype}{Character. Torch dtype: \code{"float32"} or \code{"float64"}. Default:
\code{"float32"}. Float32 halves memory usage with negligible precision loss.}

\item{convergence_tol}{Numeric. Relative change in epoch loss below which
the optimizer considers the solution converged. Default: 1e-4.}

\item{patience}{Integer. Number of consecutive epochs with no improvement
(at minimum learning rate) before early stopping. The LR scheduler
uses \code{2 * patience} as its own patience. Default: 50.}

\item{barrier_mu}{Numeric. Strength of the log-barrier penalty that
prevents any census tract from receiving zero predicted voters. The
penalty term is \code{-barrier_mu * sum(log(V_hat_total))}. Set to 0 to
disable. Default: 10.}

\item{alpha_min}{Numeric. Lower bound for alpha values. The
reparameterization becomes \code{alpha = alpha_min + softplus(theta)},
ensuring all alpha values are at least \code{alpha_min}. Default: 1,
which restricts decay to linear or steeper (alpha >= 1 matches
physical travel-time behavior in urban settings). Set to 0 to
remove the lower bound, or to 2 for inverse-square minimum.}

\item{offset}{Numeric. Value added to travel times before exponentiation.
Default: 1.}

\item{verbose}{Logical. Print progress messages? Default: TRUE.}
}
\value{
A list of class \code{"interpElections_optim"} with components:
\describe{
\item{alpha}{Numeric matrix [n x k]. Optimal per-tract-per-bracket
decay parameters. Each row is a census tract, each column is a
demographic bracket. Inactive brackets (zero population or voters)
are filled with 1.}
\item{value}{Numeric. Objective function value at optimum (Poisson
deviance).}
\item{W}{Numeric matrix [n x m]. Column-normalized weight matrix
from the best-epoch. Use directly for interpolation
via \verb{W \\\%*\\\% data}. Column sums are approximately 1.}
\item{method}{Character. Method used (e.g.,
\code{"pb_sgd_colnorm_cpu"}, \code{"pb_sgd_colnorm_cuda"}).}
\item{convergence}{Integer. 0 = early-stopped (improvement plateau
detected); 1 = stopped at max_epochs.}
\item{epochs}{Integer. Number of epochs completed.}
\item{steps}{Integer. Total number of SGD gradient steps.}
\item{elapsed}{\code{difftime} object. Wall-clock time.}
\item{message}{Character. Additional information.}
\item{history}{Numeric vector. Full-dataset loss at each epoch.}
\item{grad_norm_final}{Numeric. Final gradient norm (theta-space).}
\item{grad_history}{Numeric vector. Gradient norm (theta-space) at
each epoch.}
\item{lr_history}{Numeric vector. Learning rate at each epoch.}
}
}
\description{
Optimizes the per-tract-per-bracket decay parameters that minimize the
error between interpolated values and known population counts. Uses
per-bracket SGD with column normalization and log-barrier penalty inside
torch autograd. Each demographic bracket gets its own per-tract kernel;
gradients flow through all computations. Works on both CPU and GPU
(CUDA/MPS).
}
\details{
The optimization requires the \code{torch} R package. Install it with
\code{\link[=setup_torch]{setup_torch()}} if not already available.

\strong{Parameterization}: alpha[i,b] is reparameterized as
\code{alpha = alpha_min + softplus(theta)} with \code{theta} unconstrained,
where \code{softplus(x) = log(1 + exp(x))}. With the default
\code{alpha_min = 1}, alpha is always at least 1 (inverse-distance
decay or steeper). Set \code{alpha_min = 0} for unconstrained
optimization (similar to legacy \code{exp(theta)}).

\strong{Epoch structure}: Each epoch is one full-data gradient step with
exact gradients (column sums require all tracts). The loss reported at
each epoch is the true objective evaluated on the full dataset.

Two execution paths:
\itemize{
\item \strong{CPU} (default): \code{use_gpu = FALSE} or \code{NULL}. Uses torch on CPU
device. Fast for small/medium problems (< 2000 tracts).
\item \strong{GPU}: \code{use_gpu = TRUE}. Uses CUDA or MPS. Faster for large
problems (> 2000 tracts).
}

GPU memory usage is bounded by
\code{2 * ka * n * m * bytes_per_elem}.
}
\examples{
\dontrun{
tt <- matrix(c(2, 5, 3, 4, 6, 2), nrow = 2)
pop <- matrix(c(100, 200), nrow = 2)
src <- matrix(c(80, 120, 100), nrow = 3)
result <- optimize_alpha(tt, pop, src, verbose = FALSE)
result$alpha
}

}
\seealso{
\code{\link[=use_gpu]{use_gpu()}} to toggle GPU globally, \code{\link[=compute_weight_matrix]{compute_weight_matrix()}}
to rebuild the weight matrix from pre-computed alpha,
\code{\link[=setup_torch]{setup_torch()}} to install torch.
}
