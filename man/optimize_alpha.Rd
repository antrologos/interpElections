% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/optimize.R
\name{optimize_alpha}
\alias{optimize_alpha}
\title{Find optimal decay parameters (alpha) for spatial interpolation}
\usage{
optimize_alpha(
  time_matrix,
  pop_matrix,
  source_matrix,
  row_targets = NULL,
  alpha_init = NULL,
  sinkhorn_iter = 5L,
  use_gpu = NULL,
  device = NULL,
  dtype = "float32",
  gpu_iterations = 20L,
  gpu_lr_init = 0.1,
  gpu_lr_decay = 0.6,
  gpu_grad_tol = 1e-04,
  gpu_grad_clip = 1,
  gpu_warmup_steps = 10L,
  lower_bound = 0,
  upper_bound = 20,
  offset = 1,
  verbose = TRUE
)
}
\arguments{
\item{time_matrix}{Numeric matrix [n x m]. Raw travel times.
Rows = target zones, columns = source points.}

\item{pop_matrix}{Numeric matrix [n x k]. Known population per zone,
with k demographic groups as columns.}

\item{source_matrix}{Numeric matrix [m x k]. Known counts at source
points (e.g., registered voters by age group).}

\item{row_targets}{Numeric vector of length n, or NULL. Target row sums
for Sinkhorn balancing. Each element specifies how much weight a zone
should attract, proportional to its share of total population. If NULL
(default), auto-computed as \code{rowSums(pop_matrix) / sum(pop_matrix) * m}.}

\item{alpha_init}{Numeric vector of length n, or a single value to be
recycled. Initial guess for alpha. Default: \code{rep(1, n)}.}

\item{sinkhorn_iter}{Integer. Number of Sinkhorn iterations per objective
evaluation during optimization. Higher values give more accurate balancing
but are slower. Default: 5 (sufficient for optimization; final weights
use full convergence via \code{\link[=sinkhorn_weights]{sinkhorn_weights()}}).}

\item{use_gpu}{Logical or NULL. If \code{TRUE}, use GPU (CUDA or MPS). If
\code{FALSE}, use CPU. If \code{NULL} (default), reads the package option
\code{interpElections.use_gpu} (set via \code{\link[=use_gpu]{use_gpu()}}).}

\item{device}{Character or NULL. Torch device: \code{"cuda"}, \code{"mps"}, or
\code{"cpu"}. Only used when GPU is enabled. Default: NULL (auto-detect).}

\item{dtype}{Character. Torch dtype: \code{"float32"} or \code{"float64"}. Default:
\code{"float32"}. Float32 halves memory usage with negligible precision loss.}

\item{gpu_iterations}{Integer. Number of outer ADAM phases with learning
rate decay. Default: 20.}

\item{gpu_lr_init}{Numeric. Initial ADAM learning rate. Default: 0.1.}

\item{gpu_lr_decay}{Numeric. Learning rate decay factor per phase.
Default: 0.6.}

\item{gpu_grad_tol}{Numeric. Gradient norm threshold for convergence.
Default: 1e-4.}

\item{gpu_grad_clip}{Numeric or NULL. Maximum gradient norm for clipping.
\code{NULL} disables clipping. Default: 1.0.}

\item{gpu_warmup_steps}{Integer. Linear learning rate warmup steps at
the start of phase 1. Default: 10.}

\item{lower_bound}{Numeric. Lower bound for alpha values. Default: 0.}

\item{upper_bound}{Numeric. Upper bound for alpha values. Default: 20.}

\item{offset}{Numeric. Value added to travel times before exponentiation.
Default: 1.}

\item{verbose}{Logical. Print progress messages? Default: TRUE.}
}
\value{
A list of class \code{"interpElections_optim"} with components:
\describe{
\item{alpha}{Numeric vector. Optimal alpha values.}
\item{value}{Numeric. Objective function value at optimum.}
\item{method}{Character. Method used (e.g.,
\code{"torch_adam_sinkhorn_cpu"}, \code{"torch_adam_sinkhorn_cuda"}).}
\item{convergence}{Integer. 0 = success.}
\item{iterations}{Number of ADAM steps taken.}
\item{elapsed}{\code{difftime} object. Wall-clock time.}
\item{message}{Character. Additional information.}
\item{history}{Numeric vector. Objective values at each step.}
\item{grad_norm_final}{Numeric. Final gradient norm.}
\item{row_targets}{Numeric vector. Row targets used for Sinkhorn.}
\item{sinkhorn_iter}{Integer. Sinkhorn iterations used.}
}
}
\description{
Optimizes the per-zone decay parameters that minimize the squared error
between Sinkhorn-balanced interpolated values and known population counts.
Uses torch autograd for gradient computation with ADAM optimizer on both
CPU and GPU.
}
\details{
The weight matrix is balanced via log-domain Sinkhorn iterations
(row sums proportional to population, column sums = 1) before computing
the calibration loss. Gradients are obtained by differentiating through
the unrolled Sinkhorn iterations via torch autograd.

The optimization requires the \code{torch} R package. Install it with
\code{\link[=setup_torch]{setup_torch()}} if not already available.

Two execution paths:
\itemize{
\item \strong{CPU} (default): \code{use_gpu = FALSE} or \code{NULL}. Uses torch on CPU
device. Fast for small/medium problems (< 2000 tracts).
\item \strong{GPU}: \code{use_gpu = TRUE}. Uses CUDA or MPS. Faster for large
problems (> 2000 tracts).
}

Both paths use the same ADAM optimizer with log-domain Sinkhorn.
Gradients are computed via torch autograd through the unrolled
Sinkhorn iterations.
}
\examples{
\dontrun{
tt <- matrix(c(2, 5, 3, 4, 6, 2), nrow = 2)
pop <- matrix(c(100, 200), nrow = 2)
src <- matrix(c(80, 120, 100), nrow = 3)
result <- optimize_alpha(tt, pop, src, verbose = FALSE)
result$alpha
}

}
\seealso{
\code{\link[=use_gpu]{use_gpu()}} to toggle GPU globally, \code{\link[=sinkhorn_weights]{sinkhorn_weights()}} to
build the final weight matrix, \code{\link[=sinkhorn_objective]{sinkhorn_objective()}} for the
objective function, \code{\link[=setup_torch]{setup_torch()}} to install torch.
}
